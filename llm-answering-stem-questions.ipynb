{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":54662,"databundleVersionId":6169864,"sourceType":"competition"},{"sourceId":3680037,"sourceType":"datasetVersion","datasetId":2202288},{"sourceId":4988409,"sourceType":"datasetVersion","datasetId":2893282},{"sourceId":5632975,"sourceType":"datasetVersion","datasetId":3238926},{"sourceId":6149251,"sourceType":"datasetVersion","datasetId":3526632},{"sourceId":6282487,"sourceType":"datasetVersion","datasetId":3612472},{"sourceId":6300474,"sourceType":"datasetVersion","datasetId":3520954},{"sourceId":6359012,"sourceType":"datasetVersion","datasetId":3662908},{"sourceId":6359953,"sourceType":"datasetVersion","datasetId":3663541},{"sourceId":6412625,"sourceType":"datasetVersion","datasetId":3698271},{"sourceId":6414848,"sourceType":"datasetVersion","datasetId":3699788},{"sourceId":6416468,"sourceType":"datasetVersion","datasetId":3675739},{"sourceId":6488993,"sourceType":"datasetVersion","datasetId":3749764},{"sourceId":6521141,"sourceType":"datasetVersion","datasetId":3770056},{"sourceId":6561651,"sourceType":"datasetVersion","datasetId":3791123},{"sourceId":6572938,"sourceType":"datasetVersion","datasetId":3600418},{"sourceId":7219250,"sourceType":"datasetVersion","datasetId":612177},{"sourceId":142033605,"sourceType":"kernelVersion"},{"sourceId":142721048,"sourceType":"kernelVersion"},{"sourceId":142734271,"sourceType":"kernelVersion"},{"sourceId":144700262,"sourceType":"kernelVersion"}],"dockerImageVersionId":30559,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Installing Dependencies**","metadata":{}},{"cell_type":"code","source":"!cp /kaggle/input/datasets-wheel/datasets-2.14.4-py3-none-any.whl /kaggle/working\n!pip install  /kaggle/working/datasets-2.14.4-py3-none-any.whl\n!pip install -U /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers\n!pip install -U /kaggle/working/sentence-transformers\n!pip install -U /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\n\n!pip install --no-index --no-deps /kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl\n!cp -r /kaggle/input/stem-wiki-cohere-no-emb /kaggle/working\n!cp -r /kaggle/input/all-paraphs-parsed-expanded /kaggle/working/","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-18T20:27:38.565041Z","iopub.execute_input":"2023-12-18T20:27:38.565583Z","iopub.status.idle":"2023-12-18T20:30:37.923743Z","shell.execute_reply.started":"2023-12-18T20:27:38.565556Z","shell.execute_reply":"2023-12-18T20:30:37.922410Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Processing ./datasets-2.14.4-py3-none-any.whl\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (1.23.5)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (11.0.0)\nRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (2.0.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (3.3.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (2023.9.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (3.8.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (6.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.4) (3.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.4) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets==2.14.4) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.4) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.4) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.4) (2023.7.22)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.4) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.4) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.4) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.14.4) (1.16.0)\nInstalling collected packages: datasets\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\nSuccessfully installed datasets-2.14.4\nProcessing /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nInstalling collected packages: faiss-gpu\nSuccessfully installed faiss-gpu-1.7.2\nProcessing ./sentence-transformers\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (4.33.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (4.66.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.15.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.23.5)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (1.11.2)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.1.99)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers==2.2.2) (0.16.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.12.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2023.9.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.6.3)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (21.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2023.6.3)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.3.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->sentence-transformers==2.2.2) (1.16.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==2.2.2) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==2.2.2) (3.1.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence-transformers==2.2.2) (9.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2023.7.22)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\nBuilding wheels for collected packages: sentence-transformers\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=126125 sha256=f88b5c6647fe11593faf9a0fbf7063de5bc4a2564ea894bf60d9adacd00e45ec\n  Stored in directory: /root/.cache/pip/wheels/6c/ea/76/d9a930b223b1d3d5d6aff69458725316b0fe205b854faf1812\nSuccessfully built sentence-transformers\nInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-2.2.2\nProcessing /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\nInstalling collected packages: blingfire\nSuccessfully installed blingfire-0.1.8\nProcessing /kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.33.0\n    Uninstalling transformers-4.33.0:\n      Successfully uninstalled transformers-4.33.0\nSuccessfully installed transformers-4.31.0\nProcessing /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\nInstalling collected packages: peft\nSuccessfully installed peft-0.4.0\nProcessing /kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl\nInstalling collected packages: trl\nSuccessfully installed trl-0.5.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Importing Necessary Libraries**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nfrom datasets import load_dataset, load_from_disk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport torch\nfrom transformers import LongformerTokenizer, LongformerForMultipleChoice\nimport transformers\nimport pandas as pd\nimport pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport unicodedata\nimport os\nimport gc\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer","metadata":{"execution":{"iopub.status.busy":"2023-12-18T20:30:37.925951Z","iopub.execute_input":"2023-12-18T20:30:37.926284Z","iopub.status.idle":"2023-12-18T20:30:54.190367Z","shell.execute_reply.started":"2023-12-18T20:30:37.926254Z","shell.execute_reply":"2023-12-18T20:30:54.189517Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Implementing RAG**","metadata":{}},{"cell_type":"code","source":"def SplitList(mylist, chunk_size):\n    return [mylist[offs:offs+chunk_size] for offs in range(0, len(mylist), chunk_size)]\ndef get_relevant_documents_parsed(df_valid):\n    df_chunk_size=600\n    paraphs_parsed_dataset = load_from_disk(\"/kaggle/working/all-paraphs-parsed-expanded\")\n    modified_texts = paraphs_parsed_dataset.map(lambda example:\n                                             {'temp_text':\n                                              f\"{example['title']} {example['section']} {example['text']}\".replace('\\n',\" \").replace(\"'\",\"\")},\n                                             num_proc=2)[\"temp_text\"]\n    \n    all_articles_indices = []\n    all_articles_values = []\n    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n    \n        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n        all_articles_indices.append(articles_indices)\n        all_articles_values.append(merged_top_scores)\n        \n    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n    \n    top_per_query = article_indices_array.shape[1]\n    articles_flatten = [(\n                         articles_values_array[index],\n                         paraphs_parsed_dataset[idx.item()][\"title\"],\n                         paraphs_parsed_dataset[idx.item()][\"text\"],\n                        )\n                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n    retrieved_articles = SplitList(articles_flatten, top_per_query)\n    return retrieved_articles\ndef get_relevant_documents(df_valid):\n    df_chunk_size=800\n    \n    cohere_dataset_filtered = load_from_disk(\"/kaggle/working/stem-wiki-cohere-no-emb\")\n    modified_texts = cohere_dataset_filtered.map(lambda example:\n                                             {'temp_text':\n                                              unicodedata.normalize(\"NFKD\", f\"{example['title']} {example['text']}\").replace('\"',\"\")},\n                                             num_proc=2)[\"temp_text\"]\n    \n    all_articles_indices = []\n    all_articles_values = []\n    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n    \n        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n        all_articles_indices.append(articles_indices)\n        all_articles_values.append(merged_top_scores)\n        \n    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n    \n    top_per_query = article_indices_array.shape[1]\n    articles_flatten = [(\n                         articles_values_array[index],\n                         cohere_dataset_filtered[idx.item()][\"title\"],\n                         unicodedata.normalize(\"NFKD\", cohere_dataset_filtered[idx.item()][\"text\"]),\n                        )\n                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n    retrieved_articles = SplitList(articles_flatten, top_per_query)\n    return retrieved_articles\ndef retrieval(df_valid, modified_texts):\n    \n    corpus_df_valid = df_valid.apply(lambda row:\n                                     f'{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"A\"]}\\n{row[\"B\"]}\\n{row[\"C\"]}\\n{row[\"D\"]}\\n{row[\"E\"]}',\n                                     axis=1).values\n    vectorizer1 = TfidfVectorizer(ngram_range=(1,4),\n                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n                                 stop_words=stop_words,\n                                 strip_accents=\"unicode\",sublinear_tf=True)\n    vectorizer1.fit(corpus_df_valid)\n    vocab_df_valid = vectorizer1.get_feature_names_out()\n    vectorizer = TfidfVectorizer(ngram_range=(1,4),\n                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n                                 stop_words=stop_words,\n                                 vocabulary=vocab_df_valid,\n                                strip_accents=\"unicode\",sublinear_tf=True)\n    vectorizer.fit(modified_texts[:500000])\n    corpus_tf_idf = vectorizer.transform(corpus_df_valid)\n    \n\n    print(f\"length of vectorizer vocab is {len(vectorizer.get_feature_names_out())}\")\n\n    chunk_size = 100000\n    top_per_chunk = 10\n    top_per_query = 10\n\n    all_chunk_top_indices = []\n    all_chunk_top_values = []\n\n    for idx in tqdm(range(0, len(modified_texts), chunk_size)):\n        wiki_vectors = vectorizer.transform(modified_texts[idx: idx+chunk_size])\n        temp_scores = (corpus_tf_idf * wiki_vectors.T).toarray()\n        chunk_top_indices = temp_scores.argpartition(-top_per_chunk, axis=1)[:, -top_per_chunk:]\n        chunk_top_values = temp_scores[np.arange(temp_scores.shape[0])[:, np.newaxis], chunk_top_indices]\n\n        all_chunk_top_indices.append(chunk_top_indices + idx)\n        all_chunk_top_values.append(chunk_top_values)\n\n    top_indices_array = np.concatenate(all_chunk_top_indices, axis=1)\n    top_values_array = np.concatenate(all_chunk_top_values, axis=1)\n    \n    merged_top_scores = np.sort(top_values_array, axis=1)[:,-top_per_query:]\n    merged_top_indices = top_values_array.argsort(axis=1)[:,-top_per_query:]\n    articles_indices = top_indices_array[np.arange(top_indices_array.shape[0])[:, np.newaxis], merged_top_indices]\n    \n    return articles_indices, merged_top_scores\ndef prepare_answering_input(\n        tokenizer, \n        question,  \n        options,   \n        context,   \n        max_seq_length=700,\n        ):\n        c_plus_q   = context + '\\n' + tokenizer.bos_token + '\\n' + question\n        c_plus_q_4 = [c_plus_q] * len(options)\n        tokenized_examples = tokenizer(\n        c_plus_q_4, options,\n        max_length=max_seq_length,\n        padding=\"longest\",\n        truncation=False,\n        return_tensors=\"pt\",\n        )\n        input_ids = tokenized_examples['input_ids'].unsqueeze(0)\n        attention_mask = tokenized_examples['attention_mask'].unsqueeze(0)\n        example_encoded = {\n            \"input_ids\": input_ids.to(model.device.index),\n            \"attention_mask\": attention_mask.to(model.device.index),\n        }\n        return example_encoded","metadata":{"execution":{"iopub.status.busy":"2023-12-18T20:30:54.191598Z","iopub.execute_input":"2023-12-18T20:30:54.191869Z","iopub.status.idle":"2023-12-18T20:30:54.219825Z","shell.execute_reply.started":"2023-12-18T20:30:54.191846Z","shell.execute_reply":"2023-12-18T20:30:54.218889Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"stop_words = ['each', 'you', 'the', 'use', 'used',\n                  'where', 'themselves', 'nor', \"it's\", 'how', \"don't\", 'just', 'your',\n                  'about', 'himself', 'with', \"weren't\", 'hers', \"wouldn't\", 'more', 'its', 'were',\n                  'his', 'their', 'then', 'been', 'myself', 're', 'not',\n                  'ours', 'will', 'needn', 'which', 'here', 'hadn', 'it', 'our', 'there', 'than',\n                  'most', \"couldn't\", 'both', 'some', 'for', 'up', 'couldn', \"that'll\",\n                  \"she's\", 'over', 'this', 'now', 'until', 'these', 'few', 'haven',\n                  'of', 'wouldn', 'into', 'too', 'to', 'very', 'shan', 'before', 'the', 'they',\n                  'between', \"doesn't\", 'are', 'was', 'out', 'we', 'me',\n                  'after', 'has', \"isn't\", 'have', 'such', 'should', 'yourselves', 'or', 'during', 'herself',\n                  'doing', 'in', \"shouldn't\", \"won't\", 'when', 'do', 'through', 'she',\n                  'having', 'him', \"haven't\", 'against', 'itself', 'that',\n                  'did', 'theirs', 'can', 'those',\n                  'own', 'so', 'and', 'who', \"you've\", 'yourself', 'her', 'he', 'only',\n                  'what', 'ourselves', 'again', 'had', \"you'd\", 'is', 'other',\n                  'why', 'while', 'from', 'them', 'if', 'above', 'does', 'whom',\n                  'yours', 'but', 'being', \"wasn't\", 'be']\nfrom sklearn.feature_extraction import text\nstop_words2 = text.ENGLISH_STOP_WORDS\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\nstop_words3 = set(nlp.Defaults.stop_words)\nfrom gensim.parsing.preprocessing import STOPWORDS\nstop_words4 = set(STOPWORDS)\nstop_words = list(stop_words2.union(stop_words,stop_words3,stop_words4))","metadata":{"execution":{"iopub.status.busy":"2023-12-18T20:30:54.222199Z","iopub.execute_input":"2023-12-18T20:30:54.222523Z","iopub.status.idle":"2023-12-18T20:31:27.808444Z","shell.execute_reply.started":"2023-12-18T20:30:54.222497Z","shell.execute_reply":"2023-12-18T20:31:27.807651Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# **Taking Input**","metadata":{}},{"cell_type":"code","source":"prompt=input(\"Enter the question :\")\nA=input(\"Option A:\")\nB=input(\"Option B:\")\nC=input(\"Option C:\")\nD=input(\"Option D:\")\nE=input(\"Option E:\")","metadata":{"execution":{"iopub.status.busy":"2023-12-18T20:31:27.809457Z","iopub.execute_input":"2023-12-18T20:31:27.810106Z","iopub.status.idle":"2023-12-18T20:32:50.917511Z","shell.execute_reply.started":"2023-12-18T20:31:27.810078Z","shell.execute_reply":"2023-12-18T20:32:50.916563Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter the question : Who is the best football player ?\nOption A: Lionel Messi\nOption B: Cristiano Ronaldo\nOption C: Diego Maradona\nOption D: Zinedine Zidane\nOption E: Ronaldinho Gaúcho\n"}]},{"cell_type":"code","source":"df_valid=pd.DataFrame(data=[[prompt,A,B,C,D,E]],columns=['prompt','A','B','C','D','E'])","metadata":{"execution":{"iopub.status.busy":"2023-12-18T20:32:50.918904Z","iopub.execute_input":"2023-12-18T20:32:50.919602Z","iopub.status.idle":"2023-12-18T20:32:50.926932Z","shell.execute_reply.started":"2023-12-18T20:32:50.919566Z","shell.execute_reply":"2023-12-18T20:32:50.926030Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# **Retriving Necessary Data from Wikipedia Dataset**","metadata":{}},{"cell_type":"code","source":"retrieved_articles_parsed = get_relevant_documents_parsed(df_valid)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-12-18T20:32:50.928225Z","iopub.execute_input":"2023-12-18T20:32:50.928836Z","iopub.status.idle":"2023-12-18T20:47:39.065038Z","shell.execute_reply.started":"2023-12-18T20:32:50.928804Z","shell.execute_reply":"2023-12-18T20:47:39.064023Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/2101279 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d069409e41cb4eaca4c6dc6a24e24e6f"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 0/1 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'd', 'isn', 'll', 'm', 'n', 's', 'shouldn', 't', 've', 'wasn', 'weren', 'won'] not in stop_words.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"length of vectorizer vocab is 56\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/22 [00:00<?, ?it/s]\u001b[A\n  5%|▍         | 1/22 [00:30<10:45, 30.72s/it]\u001b[A\n  9%|▉         | 2/22 [01:01<10:10, 30.53s/it]\u001b[A\n 14%|█▎        | 3/22 [01:31<09:39, 30.51s/it]\u001b[A\n 18%|█▊        | 4/22 [02:02<09:08, 30.49s/it]\u001b[A\n 23%|██▎       | 5/22 [02:32<08:39, 30.56s/it]\u001b[A\n 27%|██▋       | 6/22 [03:03<08:10, 30.64s/it]\u001b[A\n 32%|███▏      | 7/22 [03:33<07:38, 30.58s/it]\u001b[A\n 36%|███▋      | 8/22 [04:04<07:07, 30.55s/it]\u001b[A\n 41%|████      | 9/22 [04:35<06:38, 30.67s/it]\u001b[A\n 45%|████▌     | 10/22 [05:05<06:07, 30.60s/it]\u001b[A\n 50%|█████     | 11/22 [05:35<05:34, 30.44s/it]\u001b[A\n 55%|█████▍    | 12/22 [06:06<05:05, 30.53s/it]\u001b[A\n 59%|█████▉    | 13/22 [06:37<04:34, 30.48s/it]\u001b[A\n 64%|██████▎   | 14/22 [07:07<04:04, 30.61s/it]\u001b[A\n 68%|██████▊   | 15/22 [07:38<03:34, 30.59s/it]\u001b[A\n 73%|███████▎  | 16/22 [08:09<03:03, 30.57s/it]\u001b[A\n 77%|███████▋  | 17/22 [08:39<02:33, 30.61s/it]\u001b[A\n 82%|████████▏ | 18/22 [09:10<02:02, 30.62s/it]\u001b[A\n 86%|████████▋ | 19/22 [09:40<01:31, 30.61s/it]\u001b[A\n 91%|█████████ | 20/22 [10:11<01:01, 30.62s/it]\u001b[A\n 95%|█████████▌| 21/22 [10:41<00:30, 30.53s/it]\u001b[A\n100%|██████████| 22/22 [10:42<00:00, 29.20s/it]\u001b[A\n100%|██████████| 1/1 [13:15<00:00, 795.67s/it]\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"310"},"metadata":{}}]},{"cell_type":"code","source":"retrieved_articles = get_relevant_documents(df_valid)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-12-18T20:47:39.066207Z","iopub.execute_input":"2023-12-18T20:47:39.066506Z","iopub.status.idle":"2023-12-18T21:00:57.479971Z","shell.execute_reply.started":"2023-12-18T20:47:39.066480Z","shell.execute_reply":"2023-12-18T21:00:57.478880Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/2781652 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0823edcc253f496b9ea754d58120b4fd"}},"metadata":{}},{"name":"stderr","text":"  0%|          | 0/1 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"length of vectorizer vocab is 56\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n  4%|▎         | 1/28 [00:19<08:55, 19.83s/it]\u001b[A\n  7%|▋         | 2/28 [00:38<08:24, 19.39s/it]\u001b[A\n 11%|█         | 3/28 [00:57<07:55, 19.02s/it]\u001b[A\n 14%|█▍        | 4/28 [01:15<07:30, 18.75s/it]\u001b[A\n 18%|█▊        | 5/28 [01:33<07:05, 18.50s/it]\u001b[A\n 21%|██▏       | 6/28 [01:52<06:45, 18.42s/it]\u001b[A\n 25%|██▌       | 7/28 [02:10<06:25, 18.38s/it]\u001b[A\n 29%|██▊       | 8/28 [02:28<06:04, 18.23s/it]\u001b[A\n 32%|███▏      | 9/28 [02:46<05:45, 18.16s/it]\u001b[A\n 36%|███▌      | 10/28 [03:04<05:24, 18.02s/it]\u001b[A\n 39%|███▉      | 11/28 [03:21<05:05, 17.98s/it]\u001b[A\n 43%|████▎     | 12/28 [03:39<04:44, 17.80s/it]\u001b[A\n 46%|████▋     | 13/28 [03:56<04:24, 17.65s/it]\u001b[A\n 50%|█████     | 14/28 [04:14<04:06, 17.58s/it]\u001b[A\n 54%|█████▎    | 15/28 [04:31<03:47, 17.53s/it]\u001b[A\n 57%|█████▋    | 16/28 [04:48<03:28, 17.41s/it]\u001b[A\n 61%|██████    | 17/28 [05:05<03:10, 17.30s/it]\u001b[A\n 64%|██████▍   | 18/28 [05:22<02:52, 17.24s/it]\u001b[A\n 68%|██████▊   | 19/28 [05:39<02:33, 17.04s/it]\u001b[A\n 71%|███████▏  | 20/28 [05:56<02:15, 16.97s/it]\u001b[A\n 75%|███████▌  | 21/28 [06:12<01:58, 16.89s/it]\u001b[A\n 79%|███████▊  | 22/28 [06:29<01:41, 16.84s/it]\u001b[A\n 82%|████████▏ | 23/28 [06:45<01:23, 16.68s/it]\u001b[A\n 86%|████████▌ | 24/28 [07:01<01:06, 16.52s/it]\u001b[A\n 89%|████████▉ | 25/28 [07:18<00:49, 16.42s/it]\u001b[A\n 93%|█████████▎| 26/28 [07:33<00:32, 16.18s/it]\u001b[A\n 96%|█████████▋| 27/28 [07:49<00:15, 15.93s/it]\u001b[A\n100%|██████████| 28/28 [08:00<00:00, 17.18s/it]\u001b[A\n100%|██████████| 1/1 [09:37<00:00, 577.00s/it]\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"20"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Models**","metadata":{}},{"cell_type":"code","source":"model_dir = \"/kaggle/input/llm-science-run-context-2\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T21:00:57.481365Z","iopub.execute_input":"2023-12-18T21:00:57.481643Z","iopub.status.idle":"2023-12-18T21:00:57.843564Z","shell.execute_reply.started":"2023-12-18T21:00:57.481620Z","shell.execute_reply":"2023-12-18T21:00:57.842780Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model_ckpts = [\n    \"/kaggle/input/how-to-use-40k-dataset/model_v3\",\n    \"/kaggle/input/abcdefghijklmnopqrstuvwxyz\",\n    \"/kaggle/input/tarining-on-1024/model_v2\",\n    \"/kaggle/input/naman-op\",\n    \"/kaggle/input/llm-science-exam-context-v2-models/deberta-checkpoint-7600/checkpoint-7600\",\n    \"/kaggle/input/run-sci-1/model_v8\"\n]","metadata":{"execution":{"iopub.status.busy":"2023-12-18T21:00:57.847000Z","iopub.execute_input":"2023-12-18T21:00:57.847534Z","iopub.status.idle":"2023-12-18T21:00:57.851707Z","shell.execute_reply.started":"2023-12-18T21:00:57.847506Z","shell.execute_reply":"2023-12-18T21:00:57.850842Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"weights=[0.1650775630381324, 0.1766253971639686, 0.09755601447394656, 0.19799713518049877, 0.23401899276203172, 0.12872489738142195] ","metadata":{"execution":{"iopub.status.busy":"2023-12-18T21:00:57.852876Z","iopub.execute_input":"2023-12-18T21:00:57.853203Z","iopub.status.idle":"2023-12-18T21:00:57.864592Z","shell.execute_reply.started":"2023-12-18T21:00:57.853172Z","shell.execute_reply":"2023-12-18T21:00:57.863721Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# **Inference**","metadata":{}},{"cell_type":"code","source":"w=[]\ng=0\nfor i in model_ckpts:\n    print(i,':',weights[g])\n    g+=1\n    predictions = []\n    submit_ids = []\n    x=[]\n    model = AutoModelForMultipleChoice.from_pretrained(i).cuda()\n    tokenizer = AutoTokenizer.from_pretrained(i)\n    for index in tqdm(range(df_valid.shape[0])):\n        columns = df_valid.iloc[index].values\n        submit_ids.append(columns[0])\n        question =\"Answer the following questions by eliminating wrong options:\\n\"+ columns[1]\n        options = [columns[1], columns[2], columns[3], columns[4], columns[5]]\n        context1 = f\"{retrieved_articles[index][-5][2]}\\n{retrieved_articles[index][-4][2]}\\n{retrieved_articles[index][-3][2]}\\n{retrieved_articles[index][-2][2]}\\n{retrieved_articles[index][-1][2]}\\n{retrieved_articles[index][0][2]}\"\n        context2 = f\"{retrieved_articles_parsed[index][-3][2]}\\n{retrieved_articles_parsed[index][-2][2]}\\n{retrieved_articles_parsed[index][-1][2]}\"\n        inputs1 = prepare_answering_input(\n            tokenizer=tokenizer, question=question,\n            options=options, context=context1,\n            )\n        inputs2 = prepare_answering_input(\n            tokenizer=tokenizer, question=question,\n            options=options, context=context2,\n            )\n    \n        with torch.no_grad():\n            outputs1 = model(**inputs1)    \n            losses1 = -outputs1.logits[0].detach().cpu().numpy()\n            probability1 = torch.softmax(torch.tensor(-losses1), dim=-1)\n        \n        with torch.no_grad():\n            outputs2 = model(**inputs2)\n            losses2 = -outputs2.logits[0].detach().cpu().numpy()\n            probability2 = torch.softmax(torch.tensor(-losses2), dim=-1)\n        \n        probability_ = (probability1 + probability2)/2\n        x.append(probability_)\n    w.append(x)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-18T21:00:57.865721Z","iopub.execute_input":"2023-12-18T21:00:57.866340Z","iopub.status.idle":"2023-12-18T21:03:01.077155Z","shell.execute_reply.started":"2023-12-18T21:00:57.866309Z","shell.execute_reply":"2023-12-18T21:03:01.076152Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"/kaggle/input/how-to-use-40k-dataset/model_v3 : 0.1650775630381324\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:04<00:00,  4.48s/it]\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/abcdefghijklmnopqrstuvwxyz : 0.1766253971639686\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:02<00:00,  2.66s/it]\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/tarining-on-1024/model_v2 : 0.09755601447394656\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:02<00:00,  2.65s/it]\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/naman-op : 0.19799713518049877\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:02<00:00,  2.66s/it]\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/llm-science-exam-context-v2-models/deberta-checkpoint-7600/checkpoint-7600 : 0.23401899276203172\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:02<00:00,  2.65s/it]\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/run-sci-1/model_v8 : 0.12872489738142195\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:02<00:00,  2.65s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"predict=[]\nfor i in range(len(w[0])):\n    predict+=[w[0][i]*weights[0]+w[1][i]*weights[1]+w[2][i]*weights[2]+w[3][i]*weights[3]+w[4][i]*weights[4]+w[5][i]*weights[5]]","metadata":{"execution":{"iopub.status.busy":"2023-12-18T21:03:01.078533Z","iopub.execute_input":"2023-12-18T21:03:01.079152Z","iopub.status.idle":"2023-12-18T21:03:01.086187Z","shell.execute_reply.started":"2023-12-18T21:03:01.079116Z","shell.execute_reply":"2023-12-18T21:03:01.085201Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"for i in predict :\n    predict = np.array(list(\"ABCDE\"))[np.argsort(i)][-3:].tolist()[::-1]\n    predictions.append(predict)\npredictions = [\" \".join(i) for i in predictions]","metadata":{"execution":{"iopub.status.busy":"2023-12-18T21:03:01.087457Z","iopub.execute_input":"2023-12-18T21:03:01.087839Z","iopub.status.idle":"2023-12-18T21:03:01.111844Z","shell.execute_reply.started":"2023-12-18T21:03:01.087805Z","shell.execute_reply":"2023-12-18T21:03:01.110930Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(\"Final answer :\",df_valid.loc[0,predictions[0][0]])","metadata":{"execution":{"iopub.status.busy":"2023-12-18T21:04:57.018223Z","iopub.execute_input":"2023-12-18T21:04:57.018621Z","iopub.status.idle":"2023-12-18T21:04:57.024311Z","shell.execute_reply.started":"2023-12-18T21:04:57.018585Z","shell.execute_reply":"2023-12-18T21:04:57.023403Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Final answer : Zinedine Zidane\n","output_type":"stream"}]}]}