{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Load Valid Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-18T17:20:26.408174Z","iopub.status.busy":"2023-09-18T17:20:26.407654Z","iopub.status.idle":"2023-09-18T17:20:45.555893Z","shell.execute_reply":"2023-09-18T17:20:45.554936Z","shell.execute_reply.started":"2023-09-18T17:20:26.408122Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","This script demonstrates how to use a 40k dataset for fine-tuning a transformer model.\n","It imports necessary libraries, sets environment variables, and defines various parameters for training.\n","The script also specifies the Hugging Face model to be used for fine-tuning.\n","\"\"\"\n","\n","import os\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n","\n","from typing import Optional, Union\n","import pandas as pd, numpy as np, torch\n","from datasets import Dataset\n","from dataclasses import dataclass\n","from transformers import AutoTokenizer\n","from transformers import EarlyStoppingCallback\n","from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n","from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n","\n","VER=3\n","# TRAIN WITH SUBSET OF MMLU\n","NUM_MMLU_TRAIN_SAMPLES = 2_048\n","# PARAMETER EFFICIENT FINE TUNING\n","USE_PEFT = False\n","# NUMBER OF LAYERS TO FREEZE \n","# DEBERTA LARGE HAS TOTAL OF 24 LAYERS\n","FREEZE_LAYERS = 18\n","# BOOLEAN TO FREEZE EMBEDDINGS\n","FREEZE_EMBEDDINGS = True\n","# LENGTH OF CONTEXT PLUS QUESTION ANSWER\n","MAX_INPUT = 256\n","# HUGGING FACE MODEL\n","MODEL = 'Model_Path'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-18T17:20:45.558367Z","iopub.status.busy":"2023-09-18T17:20:45.557971Z","iopub.status.idle":"2023-09-18T17:20:45.634261Z","shell.execute_reply":"2023-09-18T17:20:45.63319Z","shell.execute_reply.started":"2023-09-18T17:20:45.558333Z"},"trusted":true},"outputs":[],"source":["df_valid = pd.read_csv('/kaggle/input/60k-data-with-context-v2/train_with_context2.csv')\n","print('Validation data size:', df_valid.shape )\n","df_valid.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Load Train Data "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-18T17:20:45.636727Z","iopub.status.busy":"2023-09-18T17:20:45.635731Z","iopub.status.idle":"2023-09-18T17:20:45.646163Z","shell.execute_reply":"2023-09-18T17:20:45.645196Z","shell.execute_reply.started":"2023-09-18T17:20:45.636691Z"},"trusted":true},"outputs":[],"source":["# FUNCTIONS TO ADD A NEW RANDOM WRONG CHOICE\n","def make_random_4_from_3(row):\n","    \"\"\"\n","    Randomly selects a move from a given row and creates a new column 'D' with the selected move.\n","    Then, randomly selects another move and replaces the selected move with the new move.\n","    If the selected move is the correct answer, updates the answer to 'D'.\n","    \n","    Args:\n","        row: A pandas DataFrame row containing the moves and the correct answer.\n","        \n","    Returns:\n","        The modified row with the new column 'D' and updated moves and answer.\n","    \"\"\"\n","    wrong = [x for x in ['A','B','C'] if x != row.answer]\n","    right = [row.answer]\n","    move = np.random.choice(wrong*3 + right*2)\n","    row['D'] = row[move]\n","    duplicate = np.random.choice(wrong)\n","    row[move] = row[duplicate]\n","    if move==row.answer:\n","        row.answer = 'D'\n","    return row\n","\n","def make_random_5_from_4(row):\n","    \"\"\"\n","    Modifies the given row by randomly selecting a wrong answer choice and replacing it with a duplicate answer choice.\n","    If the selected wrong answer choice is the same as the correct answer, the correct answer is replaced with 'E'.\n","\n","    Args:\n","        row: A pandas DataFrame row representing a question and its answer choices.\n","\n","    Returns:\n","        The modified row with the answer choices updated.\n","    \"\"\"\n","    wrong = [x for x in ['A','B','C','D'] if x != row.answer]\n","    right = [row.answer]\n","    move = np.random.choice(wrong*4 + right*3)\n","    row['E'] = row[move]\n","    duplicate = np.random.choice(wrong)\n","    row[move] = row[duplicate]\n","    if move==row.answer:\n","        row.answer = 'E'\n","    return row"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-18T17:20:45.649699Z","iopub.status.busy":"2023-09-18T17:20:45.649049Z","iopub.status.idle":"2023-09-18T17:20:51.049621Z","shell.execute_reply":"2023-09-18T17:20:51.04868Z","shell.execute_reply.started":"2023-09-18T17:20:45.649666Z"},"trusted":true},"outputs":[],"source":["# LOAD 3 DATASETS AND FILTER\n","\"\"\"\n","Load three datasets and apply filters to each dataset.\n","\"\"\"\n","\n","MMLU = pd.read_csv('/kaggle/input/40k-data-with-context-v2/MMLU_17k_with_context2.csv')\n","MMLU = MMLU.loc[MMLU.is_question].sample(NUM_MMLU_TRAIN_SAMPLES, random_state=42)\n","print('MMLU shape',MMLU.shape)\n","\n","ScienceQA = pd.read_csv('/kaggle/input/40k-data-with-context-v2/ScienceQA_with_context2.csv')\n","ScienceQA_3 = ScienceQA.loc[ScienceQA.image.isna() & (ScienceQA.ct==3)].drop(columns=['D','E'])\n","ScienceQA_4 = ScienceQA.loc[ScienceQA.image.isna() & (ScienceQA.ct==4)].drop(columns=['E'])\n","print('ScienceQA_3 shape',ScienceQA_3.shape)\n","print('ScienceQA_4 shape',ScienceQA_4.shape)\n","\n","OpenBook = pd.read_csv('/kaggle/input/40k-data-with-context-v2/OpenBook_with_context2.csv')\n","OpenBook = OpenBook.loc[OpenBook.is_question]\n","print('OpenBook shape',OpenBook.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-18T17:20:51.051737Z","iopub.status.busy":"2023-09-18T17:20:51.051383Z","iopub.status.idle":"2023-09-18T17:20:59.409485Z","shell.execute_reply":"2023-09-18T17:20:59.408328Z","shell.execute_reply.started":"2023-09-18T17:20:51.051704Z"},"trusted":true},"outputs":[],"source":["# PREPROCESS AND CONCATENATE 3 DATASETS\n","COLS = [c for c in MMLU.columns if c != 'is_question']\n","ScienceQA_3 = ScienceQA_3.apply(make_random_4_from_3,axis=1)\n","ScienceQA_3= ScienceQA_3[COLS]\n","ScienceQA_4= ScienceQA_4[COLS]\n","OpenBook = OpenBook[COLS]\n","df_train = pd.concat([MMLU,ScienceQA_3,ScienceQA_4,OpenBook],axis=0,ignore_index=True)\n","df_train = df_train.apply(make_random_5_from_4,axis=1)\n","df_train = df_train.sample(frac=1).reset_index(drop=True)\n","df_train = df_train[['prompt','context','A','B','C','D','E','answer']]\n","df_train = df_train.fillna('')\n","print('Train shape', df_train.shape )\n","df_train.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Data Loader"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-18T17:20:59.411544Z","iopub.status.busy":"2023-09-18T17:20:59.411067Z","iopub.status.idle":"2023-09-18T17:20:59.427173Z","shell.execute_reply":"2023-09-18T17:20:59.425864Z","shell.execute_reply.started":"2023-09-18T17:20:59.411506Z"},"trusted":true},"outputs":[],"source":["# Define a dictionary to map options to their corresponding indices\n","option_to_index = {option: idx for idx, option in enumerate('ABCDE')}\n","\n","# Define a dictionary to map indices back to their corresponding options\n","index_to_option = {v: k for k,v in option_to_index.items()}\n","\n","def preprocess(example):\n","    \"\"\"\n","    Preprocesses a single example by tokenizing the context and prompt-answer pairs.\n","    \n","    Args:\n","        example (dict): A dictionary containing the example data with keys 'context', 'prompt', 'answer'.\n","        \n","    Returns:\n","        dict: A tokenized example with keys 'input_ids', 'attention_mask', 'token_type_ids', 'labels'.\n","    \"\"\"\n","    # Repeat the context for each option\n","    first_sentence = [ \"[CLS] \" + example['context'] ] * 5\n","    \n","    # Create the second sentences by combining the prompt and each option\n","    second_sentences = [\" #### \" + example['prompt'] + \" [SEP] \" + example[option] + \" [SEP]\" for option in 'ABCDE']\n","    \n","    # Tokenize the example using the tokenizer\n","    tokenized_example = tokenizer(first_sentence, second_sentences, truncation='only_first', \n","                                  max_length=MAX_INPUT, add_special_tokens=False)\n","    \n","    # Map the answer option to its corresponding index\n","    tokenized_example['label'] = option_to_index[example['answer']]\n","    \n","    return tokenized_example\n","\n","@dataclass\n","class DataCollatorForMultipleChoice:\n","    \"\"\"\n","    Data collator for multiple choice tasks.\n","    \"\"\"\n","    tokenizer: PreTrainedTokenizerBase\n","    padding: Union[bool, str, PaddingStrategy] = True\n","    max_length: Optional[int] = None\n","    pad_to_multiple_of: Optional[int] = None\n","    \n","    def __call__(self, features):\n","        \"\"\"\n","        Collates a list of tokenized examples into a batch.\n","        \n","        Args:\n","            features (List[dict]): A list of tokenized examples, each containing keys 'input_ids', 'attention_mask', 'token_type_ids', 'labels'.\n","        \n","        Returns:\n","            dict: A batch of tokenized examples with keys 'input_ids', 'attention_mask', 'token_type_ids', 'labels'.\n","        \"\"\"\n","        # Determine the name of the label key based on the presence of 'label' or 'labels' in the features\n","        label_name = 'label' if 'label' in features[0].keys() else 'labels'\n","        \n","        # Extract the labels from the features\n","        labels = [feature.pop(label_name) for feature in features]\n","        \n","        # Get the batch size and number of choices\n","        batch_size = len(features)\n","        num_choices = len(features[0]['input_ids'])\n","        \n","        # Flatten the features list\n","        flattened_features = [\n","            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n","        ]\n","        flattened_features = sum(flattened_features, [])\n","        \n","        # Pad the flattened features to create a batch\n","        batch = self.tokenizer.pad(\n","            flattened_features,\n","            padding=self.padding,\n","            max_length=self.max_length,\n","            pad_to_multiple_of=self.pad_to_multiple_of,\n","            return_tensors='pt',\n","        )\n","        \n","        # Reshape the batch to match the original structure\n","        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n","        \n","        # Add the labels to the batch\n","        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n","        \n","        return batch"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-18T17:20:59.429125Z","iopub.status.busy":"2023-09-18T17:20:59.428753Z","iopub.status.idle":"2023-09-18T17:21:02.306081Z","shell.execute_reply":"2023-09-18T17:21:02.305132Z","shell.execute_reply.started":"2023-09-18T17:20:59.429091Z"},"trusted":true},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","dataset_valid = Dataset.from_pandas(df_valid)\n","dataset = Dataset.from_pandas(df_train)\n","dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-18T17:21:02.307959Z","iopub.status.busy":"2023-09-18T17:21:02.307621Z","iopub.status.idle":"2023-09-18T17:23:10.512165Z","shell.execute_reply":"2023-09-18T17:23:10.511132Z","shell.execute_reply.started":"2023-09-18T17:21:02.307926Z"},"trusted":true},"outputs":[],"source":["tokenized_dataset_valid = dataset_valid.map(preprocess, remove_columns=['prompt', 'context', 'A', 'B', 'C', 'D', 'E', 'answer'])\n","tokenized_dataset = dataset.map(preprocess, remove_columns=['prompt', 'context', 'A', 'B', 'C', 'D', 'E', 'answer'],num_proc=2)\n","tokenized_dataset"]},{"cell_type":"markdown","metadata":{},"source":["# Build Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-18T17:23:10.514442Z","iopub.status.busy":"2023-09-18T17:23:10.514053Z","iopub.status.idle":"2023-09-18T17:23:18.739687Z","shell.execute_reply":"2023-09-18T17:23:18.738654Z","shell.execute_reply.started":"2023-09-18T17:23:10.514403Z"},"trusted":true},"outputs":[],"source":["model = AutoModelForMultipleChoice.from_pretrained(MODEL)"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-09-18T17:23:18.744125Z","iopub.status.busy":"2023-09-18T17:23:18.743823Z","iopub.status.idle":"2023-09-18T17:23:18.749701Z","shell.execute_reply":"2023-09-18T17:23:18.748608Z","shell.execute_reply.started":"2023-09-18T17:23:18.744099Z"},"trusted":true},"outputs":[],"source":["if USE_PEFT:\n","    !pip install --no-index --no-deps /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-18T17:23:18.751877Z","iopub.status.busy":"2023-09-18T17:23:18.751245Z","iopub.status.idle":"2023-09-18T17:23:18.790712Z","shell.execute_reply":"2023-09-18T17:23:18.789573Z","shell.execute_reply.started":"2023-09-18T17:23:18.751841Z"},"trusted":true},"outputs":[],"source":["# Check if PEFT is enabled\n","if USE_PEFT:\n","    print('We are using PEFT.')\n","    \n","    # Import necessary libraries for PEFT\n","    from peft import LoraConfig, get_peft_model, TaskType\n","    \n","    # Configure PEFT parameters\n","    peft_config = LoraConfig(\n","        r=8, lora_alpha=4, task_type=TaskType.SEQ_CLS, lora_dropout=0.1, \n","        bias=\"none\", inference_mode=False, \n","        target_modules=[\"query_proj\", \"value_proj\"],\n","        modules_to_save=['classifier','pooler'],\n","    )\n","    \n","    # Get the PEFT model\n","    model = get_peft_model(model, peft_config)\n","    \n","    # Print the trainable parameters of the model\n","    model.print_trainable_parameters()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-18T17:23:18.792724Z","iopub.status.busy":"2023-09-18T17:23:18.792235Z","iopub.status.idle":"2023-09-18T17:23:18.804377Z","shell.execute_reply":"2023-09-18T17:23:18.803345Z","shell.execute_reply.started":"2023-09-18T17:23:18.792689Z"},"trusted":true},"outputs":[],"source":["# Freeze the embeddings if FREEZE_EMBEDDINGS is True\n","if FREEZE_EMBEDDINGS:\n","    print('Freezing embeddings.')\n","    for param in model.deberta.embeddings.parameters():\n","        param.requires_grad = False\n","\n","# Freeze the specified number of layers if FREEZE_LAYERS is greater than 0\n","if FREEZE_LAYERS > 0:\n","    print(f'Freezing {FREEZE_LAYERS} layers.')\n","    for layer in model.deberta.encoder.layer[:FREEZE_LAYERS]:\n","        for param in layer.parameters():\n","            param.requires_grad = False"]},{"cell_type":"markdown","metadata":{},"source":["# MAP@3 Metric\n","The competition metric is MAP@3 therefore we will make a custom code to add to Hugging Face's trainer. Discussion [here][1]\n","\n","[1]: https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/435602"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-18T17:23:18.806573Z","iopub.status.busy":"2023-09-18T17:23:18.806112Z","iopub.status.idle":"2023-09-18T17:23:18.815239Z","shell.execute_reply":"2023-09-18T17:23:18.81411Z","shell.execute_reply.started":"2023-09-18T17:23:18.806542Z"},"trusted":true},"outputs":[],"source":["def map_at_3(predictions, labels):\n","    \"\"\"\n","    Calculates the Mean Average Precision at 3 (MAP@3) score.\n","\n","    Args:\n","        predictions (numpy.ndarray): Array of predicted values.\n","        labels (numpy.ndarray): Array of true labels.\n","\n","    Returns:\n","        float: The MAP@3 score.\n","    \"\"\"\n","    map_sum = 0\n","    pred = np.argsort(-1*np.array(predictions),axis=1)[:,:3]\n","    for x,y in zip(pred,labels):\n","        z = [1/i if y==j else 0 for i,j in zip([1,2,3],x)]\n","        map_sum += np.sum(z)\n","    return map_sum / len(predictions)\n","\n","def compute_metrics(p):\n","    predictions = p.predictions.tolist()\n","    labels = p.label_ids.tolist()\n","    return {\"map@3\": map_at_3(predictions, labels)}"]},{"cell_type":"markdown","metadata":{},"source":["# Train and Save "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-18T17:23:18.817161Z","iopub.status.busy":"2023-09-18T17:23:18.816706Z","iopub.status.idle":"2023-09-18T17:23:18.89861Z","shell.execute_reply":"2023-09-18T17:23:18.897631Z","shell.execute_reply.started":"2023-09-18T17:23:18.817113Z"},"trusted":true},"outputs":[],"source":["# Define the TrainingArguments with customizable parameters for training a model\n","training_args = TrainingArguments(\n","    warmup_ratio=0.1,  # The ratio of warmup steps to total training steps\n","    learning_rate=1e-5,  # The learning rate for the optimizer\n","    per_device_train_batch_size=1,  # The batch size per device for training\n","    per_device_eval_batch_size=2,  # The batch size per device for evaluation\n","    num_train_epochs=2,  # The number of training epochs\n","    report_to='none',  # The destination to send training reports\n","    output_dir=f'./checkpoints_{VER}',  # The output directory for saving checkpoints and logs\n","    overwrite_output_dir=True,  # Whether to overwrite the output directory if it already exists\n","    fp16=True,  # Whether to use mixed precision training with float16\n","    gradient_accumulation_steps=8,  # The number of steps to accumulate gradients before performing an update\n","    logging_steps=75,  # The number of steps between logging training metrics\n","    evaluation_strategy='steps',  # The strategy for evaluating during training\n","    eval_steps=75,  # The number of steps between evaluations\n","    save_strategy=\"steps\",  # The strategy for saving checkpoints\n","    save_steps=75,  # The number of steps between saving checkpoints\n","    load_best_model_at_end=False,  # Whether to load the best model at the end of training\n","    metric_for_best_model='map@3',  # The metric to use for determining the best model\n","    lr_scheduler_type='cosine',  # The type of learning rate scheduler to use\n","    weight_decay=0.01,  # The weight decay rate for regularization\n","    save_total_limit=2,  # The maximum number of checkpoints to keep\n",")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-18T17:23:18.902912Z","iopub.status.busy":"2023-09-18T17:23:18.901953Z"},"trusted":true},"outputs":[],"source":["# Create a Trainer object for training the model\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    tokenizer=tokenizer,\n","    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n","    train_dataset=tokenized_dataset,\n","    eval_dataset=tokenized_dataset_valid,\n","    compute_metrics=compute_metrics,\n","    #callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",")\n","\n","# Train the model\n","trainer.train()\n","\n","# Save the trained model\n","trainer.save_model(f'model_v{VER}')"]},{"cell_type":"markdown","metadata":{},"source":["# Verify Saved Model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["del model, trainer\n","if USE_PEFT:\n","    model = AutoModelForMultipleChoice.from_pretrained(MODEL)\n","    model = get_peft_model(model, peft_config)\n","    checkpoint = torch.load(f'model_v{VER}/pytorch_model.bin')\n","    model.load_state_dict(checkpoint)\n","else:\n","    model = AutoModelForMultipleChoice.from_pretrained(f'model_v{VER}')\n","trainer = Trainer(model=model)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_df = pd.read_csv('/kaggle/input/60k-data-with-context-v2/train_with_context2.csv')\n","tokenized_test_dataset = Dataset.from_pandas(test_df).map(\n","        preprocess, remove_columns=['prompt', 'context', 'A', 'B', 'C', 'D', 'E'])\n","\n","test_predictions = trainer.predict(tokenized_test_dataset).predictions\n","predictions_as_ids = np.argsort(-test_predictions, 1)\n","predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\n","predictions_as_string = test_df['prediction'] = [\n","    ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n","]"]},{"cell_type":"markdown","metadata":{},"source":["# Compute Validation Score"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","def precision_at_k(r, k):\n","    \"\"\"\n","    Calculate the precision at k.\n","    \n","    Parameters:\n","    r (list): The list of relevance scores.\n","    k (int): The value of k.\n","    \n","    Returns:\n","    float: The precision at k.\n","    \"\"\"\n","    assert k <= len(r)\n","    assert k != 0\n","    return sum(int(x) for x in r[:k]) / k\n","\n","def MAP_at_3(predictions, true_items):\n","    \"\"\"\n","    Calculate the mean average precision at 3.\n","\n","    Parameters:\n","    predictions (list): A list of strings representing the predicted items for each user.\n","    true_items (list): A list of strings representing the true items for each user.\n","\n","    Returns:\n","    float: The mean average precision at 3.\n","\n","    \"\"\"\n","    U = len(predictions)\n","    map_at_3 = 0.0\n","    for u in range(U):\n","        user_preds = predictions[u].split()\n","        user_true = true_items[u]\n","        user_results = [1 if item == user_true else 0 for item in user_preds]\n","        for k in range(min(len(user_preds), 3)):\n","            map_at_3 += precision_at_k(user_results, k+1) * user_results[k]\n","    return map_at_3 / U"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["m = MAP_at_3(test_df.prediction.values, test_df.answer.values)\n","print( 'CV MAP@3 =',m )"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":6169864,"sourceId":54662,"sourceType":"competition"},{"datasetId":3600418,"sourceId":6301555,"sourceType":"datasetVersion"},{"datasetId":3686746,"sourceId":6402466,"sourceType":"datasetVersion"},{"datasetId":3698271,"sourceId":6412625,"sourceType":"datasetVersion"},{"datasetId":3746360,"sourceId":6494879,"sourceType":"datasetVersion"}],"dockerImageVersionId":30554,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
